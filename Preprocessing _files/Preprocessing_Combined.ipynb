{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shreyasi Combined\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "    The order of pre-processing is the following:\n",
    "    Basic (Lower case, digit removal etc)\n",
    "    short-form replacer\n",
    "    stopwords removal\n",
    "    Lemmatization\n",
    "    print ngrams \n",
    "    and use it to tokenise finally\n",
    "\n",
    "'''\n",
    "\n",
    "# ser = data['project description']\n",
    "\n",
    "def basic(ser)\n",
    "\n",
    "    # removing everything except alphabets`\n",
    "    ser = ser.str.replace(\"[^a-zA-Z0-9-#]\", \" \")\n",
    "    ser = ser.str.replace('\\d+', '')\n",
    "\n",
    "    # make all text lowercase\n",
    "    ser = ser.apply(lambda x: x.lower())\n",
    "\n",
    "    return ser\n",
    "\n",
    "\n",
    "def replacer(ser):\n",
    "    import replacers\n",
    "    # ser = data['project description']\n",
    "\n",
    "    from replacers import RegexpReplacer\n",
    "    replacer = RegexpReplacer()\n",
    "    for i in range(len(ser)):\n",
    "        ser[i] = replacer.replace(ser[i])\n",
    "\n",
    "    return ser\n",
    "\n",
    "\n",
    "def stopwords(ser):\n",
    "    from nltk.corpus import webtext\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    new_stopwords = ['using', 'used', 'develop', 'developed', 'study', 'studied', 'given', 'review', 'value', 'done',\n",
    "                     'performed', 'implement', 'implementation', 'application',\n",
    "                     'guide', 'prof', 'predict', 'technology', 'jupyter', 'notebook', 'achieved', 'different',\n",
    "                     'technique', 'create', 'created', 'python',\n",
    "                     'implemented', 'worked', 'code', 'google', 'colab', 'trained', 'technologies', 'proposed',\n",
    "                     'performed', \n",
    "                     'build', 'built', 'technology', 'implemented', 'worked']\n",
    "\n",
    "    new_stopwords = []\n",
    "\n",
    "    new_stopwords_list = list(stop_words.union(new_stopwords))\n",
    "\n",
    "    from nltk.tokenize import MWETokenizer\n",
    "    tokenizer = MWETokenizer()\n",
    "\n",
    "    tokenized_doc = ser.apply(lambda x: tokenizer.tokenize(x.split()))\n",
    "    # remove stop-words\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in new_stopwords_list])\n",
    "\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "def lemmatizer(tokenized_doc):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for doc in tokenized_doc:\n",
    "        for i in range(len(doc)):\n",
    "            doc[i] = lemmatizer.lemmatize(str(doc[i]))\n",
    "\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "def print_ngrams(tokenized_doc):\n",
    "    import itertools\n",
    "    words = list(itertools.chain.from_iterable(tokenized_doc))\n",
    "\n",
    "    from nltk.collocations import BigramCollocationFinder\n",
    "    from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "    bcf = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "    # filter_stops = lambda w: len(w) < 3 or w in new_stopwords_list\n",
    "    filter_stops = lambda w: w in new_stopwords_list\n",
    "    bcf.apply_word_filter(filter_stops)\n",
    "\n",
    "    from nltk.collocations import TrigramCollocationFinder\n",
    "    from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "    tcf = TrigramCollocationFinder.from_words(words)\n",
    "    tcf.apply_word_filter(filter_stops)\n",
    "    tcf.apply_freq_filter(3)\n",
    "\n",
    "    bigrams = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 20)\n",
    "    trigrams = tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 20)\n",
    "    ngrams = bigrams + trigrams\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def tokenize_ngram(ngrams, tokenized_doc):\n",
    "    from nltk.tokenize import MWETokenizer\n",
    "    tokenizer = MWETokenizer(ngrams)\n",
    "\n",
    "    # de-tokenization\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(detokenized_doc)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    data = detokenized_doc\n",
    "\n",
    "    tokenized_doc = data.apply(lambda x: tokenizer.tokenize(x.split()))\n",
    "\n",
    "    # de-tokenization\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(data)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    data = detokenized_doc\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smayan-1\n",
    "\n",
    "# A Doc objectâ€™s doc.noun_chunks property allows us to iterate over the noun chunks in the document. \n",
    "# A noun chunk is a phrase that has a noun as its head. \n",
    "\n",
    "# Example for the phrase \" A noun chunk is a good phrase that has a noun as its head\"\n",
    "\n",
    "# 1. A noun chunk\n",
    "# 2. a good phrase\n",
    "# 3. a noun\n",
    "# 4. its head\n",
    "\n",
    "# Best lead so far.\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "df = pd.read_csv(\"data_final.csv\")\n",
    "df.columns = [\"project\"]\n",
    "rawlist = list(df.project)\n",
    "\n",
    "items = len(rawlist)\n",
    "\n",
    "cleaned_documents = []\n",
    "\n",
    "for item in range(items):\n",
    "    print(f\"Project {item}\")\n",
    "    text = rawlist[item]\n",
    "    doc = nlp(text)\n",
    "    phrase = \"\"\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase = phrase + \" \" + str(chunk)\n",
    "#         print(phrase)\n",
    "#         print(type(phrase))\n",
    "        print(chunk)\n",
    "#         print(type(chunk))\n",
    "    cleaned_documents.append(phrase)\n",
    "    print(\"\\n\")\n",
    "df= pd.DataFrame(cleaned_documents)\n",
    "df.to_csv(\"data_final_spacy.csv\", mode = 'a', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smayan-2\n",
    "\n",
    "## Basic Analysis\n",
    "\n",
    "# Data Loading\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/content/data_final.csv\")\n",
    "df.head()\n",
    "len(df)\n",
    "\n",
    "labels = df[\"label\"].tolist()\n",
    "labels[:10]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents = df['project'].tolist()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "cleaned_documents = []\n",
    "\n",
    "for document in documents:\n",
    "    text = document\n",
    "    print(text)\n",
    "    clean_word_list = [word for word in text.split() if word not in stoplist]\n",
    "    print(clean_word_list)\n",
    "    cleaned_text = ' '.join(clean_word_list)\n",
    "    cleaned_documents.append(cleaned_text)\n",
    "\n",
    "documents = cleaned_documents\n",
    "corpus = \" \".join(documents).lower()\n",
    "print(documents)\n",
    "\n",
    "# Data Cleaning- Removing punctuation and digits\n",
    "\n",
    "print(documents)\n",
    "import string\n",
    "\n",
    "def clean_text(corpus):\n",
    "    # Remove punctuations from the corpus\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    corpus = corpus.translate(translator)\n",
    "\n",
    "    # Remove digits from the corpus\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    corpus = corpus.translate(remove_digits)\n",
    "    return corpus\n",
    "corpus = clean_text(corpus=corpus)\n",
    "corpus[:1000]\n",
    "\n",
    "\n",
    "# Data Analysis using Spacy:\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Finding size ofnlp.max_length_length w.r.t max_length of spacy model\n",
    "len(corpus) / nlp.max_length\n",
    "\n",
    "# Spliting corpus to match the max length of spacy\n",
    "corpus_chunk = [corpus[i:i+nlp.max_length] for i in range (0,len(corpus), nlp.max_length)]\n",
    "\n",
    "# Parse each chunk of corpus with spacy\n",
    "docs = []\n",
    "for chunk in corpus_chunk:\n",
    "    doc = nlp(chunk)\n",
    "    docs.append(doc)\n",
    "    \n",
    "# Named Entity Recognition\n",
    "\n",
    "words = []\n",
    "nouns = []\n",
    "verbs = []\n",
    "people = []\n",
    "orgs = []\n",
    "\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        words.append(token.text)\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verbs.append(token.text)\n",
    "            \n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.append(token.text)\n",
    "# Finding organizations and people\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            orgs.append(ent.text)\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.append(ent.text)\n",
    "            \n",
    "print(\"Total words: \", len(words))\n",
    "print(\"Total verbs: \", len(verbs))\n",
    "print(\"Total nouns: \", len(nouns))\n",
    "print(\"Total people: \", len(people))\n",
    "print(\"Total organizations: \", len(orgs))\n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "def sort_by_frequency(data,reverse = True):\n",
    "    \"\"\" \n",
    "    Function to sord the data by its frequency\n",
    "    Returns ordered dictionary\n",
    "    Default: Sort in descending order\n",
    "        \n",
    "    \"\"\"\n",
    "    data_with_freq = dict(Counter(data))\n",
    "    data_sorted_by_freq = OrderedDict(sorted(data_with_freq.items(), key=lambda x: x[1],reverse=reverse))\n",
    "    \n",
    "    return data_sorted_by_freq\n",
    "\n",
    "words_frequency = sort_by_frequency(words)\n",
    "nouns_frequency = sort_by_frequency(nouns)\n",
    "verbs_frequency  = sort_by_frequency(verbs)\n",
    "people_frequency = sort_by_frequency(people)\n",
    "orgs_frequency = sort_by_frequency(orgs)\n",
    "\n",
    "words_frequency\n",
    "\n",
    "\n",
    "## N-Gram Analysis\n",
    "import nltk\n",
    "\n",
    "def create_bigram(tokens):\n",
    "    # Using words token generated from spacy to find bigram\n",
    "    bigrams_ = nltk.bigrams(tokens)\n",
    "    # Convert generator into list of tuples of bigram \n",
    "    return list(bigrams_)\n",
    "\n",
    "bigrams_list = create_bigram(words)\n",
    "bigrams = [\" \".join(bigram) for bigram in list(bigrams_list)]\n",
    "\n",
    "bigrams[:10]\n",
    "\n",
    "# Sorting bigram by frequency\n",
    "bigram_frequency = sort_by_frequency(bigrams)\n",
    "\n",
    "def get_top_n_from_order_dict(ordered_dict,n):\n",
    "    \"\"\" Function to find n top object from ordered dictionary\"\"\"\n",
    "    return [list(ordered_dict.items())[i] for i in range(n)]\n",
    "\n",
    "# Finding top 25 bigram\n",
    "top_bigram = get_top_n_from_order_dict(bigram_frequency,25)\n",
    "top_bigram\n",
    "\n",
    "#3Visualizing the top bigrams\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_list_of_tuple(data):\n",
    "    \"\"\"\"\"\"\n",
    "    list1, list2 = zip(*data)\n",
    "    y_pos = np.arange(len(list1))\n",
    "    plt.barh(y_pos, list2, align='center', alpha=0.5)\n",
    "    plt.yticks(y_pos, list1)\n",
    "    plt.show()\n",
    "    \n",
    "##Finding unique nouns,verbs, people, name and organization\n",
    "\n",
    "unique_nouns = set(nouns)\n",
    "unique_verbs = set(verbs)\n",
    "unique_people = set(people)\n",
    "unique_orgs = set(orgs)            \n",
    "\n",
    "# Both words Nouns\n",
    "def get_noun_noun_bigram(bigrams):\n",
    "    \"Find bigram with both word noun\"\n",
    "    NN_bigrams = []\n",
    "    for first_word,second_word in bigrams:\n",
    "        if first_word in unique_nouns and second_word in unique_nouns:\n",
    "            NN_bigrams.append(\" \".join((first_word,second_word)))\n",
    "    return NN_bigrams\n",
    "NN_bigrams = get_noun_noun_bigram(bigrams_list)\n",
    "NN_bigrams\n",
    "\n",
    "# First word verb\n",
    "def get_bigram_starting_with_verb(bigrams):\n",
    "    V_bigrams = []\n",
    "    for first_word,second_word in bigrams:\n",
    "        if first_word in unique_verbs:\n",
    "            V_bigrams.append(\" \".join((first_word,second_word)))\n",
    "            \n",
    "    return V_bigrams\n",
    "V_bigrams = get_bigram_starting_with_verb(bigrams_list)\n",
    "V_bigrams\n",
    "\n",
    "# First word organization \n",
    "def get_bigram_starting_with_organization(bigrams):\n",
    "    org_bigrams = []\n",
    "    for first_word,second_word in bigrams:\n",
    "        if first_word in unique_orgs:\n",
    "            org_bigrams.append(\" \".join((first_word,second_word)))\n",
    "    return org_bigrams\n",
    "org_bigrams = get_bigram_starting_with_organization(bigrams_list)\n",
    "org_bigrams\n",
    "\n",
    "# First word organization \n",
    "def get_bigram_starting_with_person(bigrams):\n",
    "    people_bigrams = []\n",
    "    for first_word,second_word in bigrams:\n",
    "        if first_word in unique_people:\n",
    "            people_bigrams.append(\" \".join((first_word,second_word)))\n",
    "    return people_bigrams\n",
    "people_bigrams = get_bigram_starting_with_person(bigrams_list)\n",
    "people_bigrams\n",
    "\n",
    "# Sorting the bigrams by frequency.\n",
    "\n",
    "NN_bigrams_frequency = sort_by_frequency(NN_bigrams)\n",
    "V_bigrams_frequency  = sort_by_frequency(V_bigrams)\n",
    "people_bigrams_frequency = sort_by_frequency(people_bigrams)\n",
    "org_bigrams_frequency = sort_by_frequency(org_bigrams)\n",
    "\n",
    "# Finding top 26 bigrams with both nouns\n",
    "top_25_NN_bigram = get_top_n_from_order_dict(NN_bigrams_frequency,25)\n",
    "visualize_list_of_tuple(top_25_NN_bigram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
